# On-Device Chat Moderation

<p align="center">
  <strong>Pipeline completo de modera√ß√£o de chat para iOS com ML on-device</strong><br>
  Regras + Toxicity Classifier (CoreML) + PII Detection | Zero lat√™ncia | 100% privado
</p>

---

## Vis√£o Geral

Este projeto entrega uma **solu√ß√£o completa de modera√ß√£o de chat** para iOS, incluindo:

- **Swift Package (`ChatAICore`)**: N√∫cleo de modera√ß√£o reutiliz√°vel com regras, PII detection e ML scoring
- **DemoApp**: App iOS de exemplo em SwiftUI
- **Pipeline ML (`ai/`)**: Scripts Python para treinar, avaliar e exportar modelos de toxicidade para CoreML/ONNX

**Principais Features:**
- ‚úÖ **On-device ML**: Toxicity classifier roda 100% local (CoreML)
- ‚úÖ **Regras customiz√°veis**: Keywords, regex, dom√≠nios bloqueados
- ‚úÖ **PII Detection**: Detecta telefone, email, CPF, endere√ßos, Pix, URLs
- ‚úÖ **Off-platform prevention**: Bloqueia tentativas de contato fora do app (WhatsApp, Telegram, etc.)
- ‚úÖ **Multi-categoria**: toxicity, insult, hate, threat, sexual, self_harm
- ‚úÖ **Zero lat√™ncia**: Sem chamadas de API, tudo local
- ‚úÖ **Privacy-first**: Nenhum dado do usu√°rio sai do dispositivo

## Estrutura do Projeto

```
on-device-chat-moderation/
‚îú‚îÄ ChatAICore/                    # Swift Package (biblioteca iOS)
‚îÇ  ‚îú‚îÄ Package.swift
‚îÇ  ‚îî‚îÄ Sources/ChatAICore/
‚îÇ     ‚îú‚îÄ Models/                  # Message, ModerationVerdict, ModerationPolicy
‚îÇ     ‚îú‚îÄ Services/                # ModerationService, RuleEngine, ToxicityScoring
‚îÇ     ‚îú‚îÄ ViewModel/               # ChatViewModel
‚îÇ     ‚îî‚îÄ Resources/
‚îÇ        ‚îî‚îÄ ModerationPolicy.json # Configura√ß√£o de regras (produ√ß√£o)
‚îÇ
‚îú‚îÄ DemoApp/                       # App iOS de exemplo
‚îÇ  ‚îú‚îÄ ChatAIApp.swift
‚îÇ  ‚îî‚îÄ Views/ChatView.swift
‚îÇ
‚îî‚îÄ ai/                            # Pipeline de ML (Python)
   ‚îú‚îÄ scripts/
   ‚îÇ  ‚îú‚îÄ train.py                 # Treina modelo de toxicidade
   ‚îÇ  ‚îú‚îÄ evaluate.py              # Avalia modelo no test set
   ‚îÇ  ‚îú‚îÄ export_coreml_ids.py     # Exporta para CoreML
   ‚îÇ  ‚îú‚îÄ export_onnx.py           # Exporta para ONNX
   ‚îÇ  ‚îî‚îÄ suggest_thresholds.py    # Calibra thresholds
   ‚îú‚îÄ data/                       # Datasets JSONL (train, valid, test)
   ‚îú‚îÄ artifacts/                  # Modelos gerados (CoreML, ONNX, checkpoints)
   ‚îú‚îÄ ModerationPolicy.json       # Configura√ß√£o completa (refer√™ncia)
   ‚îî‚îÄ requirements.txt
```

## Quick Start (iOS)

### Op√ß√£o 1: Rodar o DemoApp (com FakeScorer)

1. **Criar projeto iOS no Xcode**
   ```bash
   # No Xcode: File > New > Project > App (SwiftUI, iOS 15+)
   # Nome: ChatAIModerationDemo
   ```

2. **Adicionar ChatAICore como pacote local**
   - No Xcode: `File > Add Packages...`
   - Clique em `Add Local...` e selecione a pasta `ChatAICore/`
   - Em `Add to Target`, selecione seu app

3. **Copiar c√≥digo do DemoApp**
   - Copie `DemoApp/Views/ChatView.swift` para seu projeto
   - Substitua o conte√∫do de `ChatAIModerationDemoApp.swift` pelo c√≥digo de `DemoApp/ChatAIApp.swift`
   - Certifique-se de `import ChatAICore` na view

4. **Rodar no Simulator**
   ```bash
   # Command + R (iOS 15+)
   # O app funcionar√° com FakeScorer (sem ML real)
   ```

### Op√ß√£o 2: Usar com modelo CoreML real

Depois de treinar o modelo (veja se√ß√£o [Pipeline de ML](#pipeline-de-ml-treinamento)), siga:

1. **Adicionar o modelo ao app**
   ```bash
   # Arraste ai/artifacts/run1_coreml/ToxicityClassifier.mlpackage para o Xcode
   # Target Membership: marque seu app
   ```

2. **Criar CoreMLScorer**

   Crie `CoreMLScorer.swift` no seu projeto:

   ```swift
   import ChatAICore
   import CoreML

   public struct CoreMLScorer: ToxicityScoring {
       private let model: ToxicityClassifier
       private let labels: [String]

       public init() throws {
           self.model = try ToxicityClassifier(configuration: MLModelConfiguration())
           // Carregar labels do labels.json ou hardcode:
           self.labels = ["toxicity", "insult", "hate", "threat", "sexual", "self_harm"]
       }

       public func score(text: String) throws -> [String: Double] {
           let input = ToxicityClassifierInput(text: text)
           let output = try model.prediction(input: input)

           // Converter output.scores (MLMultiArray) para [String: Double]
           var scores: [String: Double] = [:]
           for (index, label) in labels.enumerated() {
               scores[label] = output.scores[index].doubleValue
           }
           return scores
       }
   }
   ```

3. **Atualizar ChatView.swift**
   ```swift
   // Trocar:
   let scorer = FakeScorer()

   // Por:
   let scorer = try! CoreMLScorer()
   ```

---

## Pipeline de ML (Treinamento)

O diret√≥rio `ai/` cont√©m todo o pipeline para treinar e exportar modelos de toxicidade.

### Setup do Ambiente Python

```bash
cd ai/
python -m venv .venv
source .venv/bin/activate  # ou .venv\Scripts\activate no Windows
pip install -r requirements.txt
```

### 1. Preparar Dados

Crie arquivos JSONL em `ai/data/` no formato:

```jsonl
{"text": "voc√™ √© idiota", "labels": {"toxicity": 1, "insult": 1}}
{"text": "obrigado pela ajuda!", "labels": {"toxicity": 0}}
```

**Categorias suportadas:**
- `toxicity` ‚Äî Toxicidade geral
- `insult` ‚Äî Insultos pessoais
- `hate` ‚Äî Discurso de √≥dio (racismo, homofobia, etc.)
- `threat` ‚Äî Amea√ßas de viol√™ncia
- `sexual` ‚Äî Conte√∫do sexual inapropriado
- `self_harm` ‚Äî Automutila√ß√£o ou suic√≠dio

### 2. Treinar Modelo

```bash
cd ai/
python scripts/train.py \
  --model distilbert-base-multilingual-cased \
  --train data/train.jsonl \
  --valid data/valid.jsonl \
  --output artifacts/run1 \
  --epochs 3 \
  --batch-size 16 \
  --lr 2e-5
```

**Par√¢metros principais:**
- `--model`: Base do HuggingFace (padr√£o: `distilbert-base-multilingual-cased`)
- `--epochs`: N√∫mero de √©pocas de treino
- `--batch-size`: Tamanho do batch
- `--lr`: Learning rate

### 3. Avaliar Modelo

```bash
python scripts/evaluate.py \
  --checkpoint artifacts/run1/checkpoint-6 \
  --test data/test.jsonl
```

Retorna m√©tricas por categoria: accuracy, precision, recall, F1-score.

### 4. Calibrar Thresholds

```bash
python scripts/suggest_thresholds.py \
  --checkpoint artifacts/run1/checkpoint-6 \
  --test data/test.jsonl
```

Sugere thresholds √≥timos para cada categoria baseado no F1-score.

### 5. Exportar para CoreML

```bash
python scripts/export_coreml_ids.py \
  --checkpoint artifacts/run1/checkpoint-6 \
  --output artifacts/run1_coreml
```

**Outputs:**
- `ToxicityClassifier.mlpackage` ‚Äî Modelo CoreML
- `labels.json` ‚Äî Lista de categorias
- `tokenizer.json`, `vocab.txt` ‚Äî Tokenizer metadata

### 6. Exportar para ONNX (Android/multi-plataforma)

```bash
python scripts/export_onnx.py \
  --checkpoint artifacts/run1/checkpoint-6 \
  --output artifacts/run1_onnx
```

**Outputs:**
- `model.onnx` ‚Äî Modelo ONNX para TensorFlow Lite/ONNX Runtime

---

## ModerationPolicy.json ‚Äî Configura√ß√£o

O arquivo `ModerationPolicy.json` define todas as regras e thresholds. Existem duas vers√µes:

- **`ChatAICore/Sources/ChatAICore/Resources/ModerationPolicy.json`** ‚Äî Vers√£o simplificada embedada no app (produ√ß√£o)
- **`ai/ModerationPolicy.json`** ‚Äî Vers√£o completa com PII patterns e off-platform rules (refer√™ncia)

### Estrutura da Pol√≠tica

```json
{
  "maxLength": 2000,
  "minToxicity": 0.80,
  "categoriesThresholds": {
    "toxicity": 0.80,
    "insult": 0.75,
    "hate": 0.70,
    "threat": 0.60,
    "sexual": 0.80,
    "self_harm": 0.60
  },
  "blockedKeywords": ["neg√≥cio por fora", "pagar fora"],
  "blockedRegex": [
    "(?i)\\b(whats?app|zapzap|zap)\\b",
    "(?i)\\b(telegram)\\b"
  ],
  "blockedDomains": ["wa.me", "t.me", "instagram.com"],
  "piiPatterns": {
    "phoneBR": "(?i)(\\+55\\s?)?(\\(?\\d{2}\\)?\\s?)?(9?\\d{4})[-\\s\\.]?\\d{4}",
    "email": "(?i)[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}",
    "cpf": "\\b\\d{3}\\.\\d{3}\\.\\d{3}-\\d{2}\\b|\\b\\d{11}\\b",
    "pix": "(?i)\\b(chave\\s*pix|pix)\\b",
    "endereco": "(?i)\\b(rua|r\\.|avenida|av\\.)\\s+[\\p{L}0-9 .'-]+\\b\\s*\\d{1,5}"
  },
  "offPlatformKeywords": [
    "me chama no whatsapp",
    "vamos fechar por fora"
  ]
}
```

### Tipos de Regras

#### 1. **Regras R√°pidas (Quick Check)**
Executadas ANTES do ML scorer para bloquear instantaneamente:

- **`blockedKeywords`**: Lista de palavras/frases proibidas
- **`blockedRegex`**: Express√µes regulares para padr√µes complexos
- **`blockedDomains`**: Dom√≠nios bloqueados (anti-phishing, off-platform)
- **`maxLength`**: Tamanho m√°ximo da mensagem

#### 2. **PII Detection (Informa√ß√µes Pessoais)**
Patterns regex para detectar dados sens√≠veis:

- **Telefone BR**: `(11) 98765-4321`, `11987654321`, `+55 11 98765-4321`
- **Email**: `usuario@exemplo.com`
- **CPF/CNPJ**: Formatados ou sem pontua√ß√£o
- **Endere√ßo**: `Rua ABC, 123` / `Av. XYZ, 456`
- **Pix**: `chave pix`, `pix: exemplo@mail.com`
- **CEP**: `12345-678`
- **URLs**: `http://site.com`, `https://exemplo.com/path`

#### 3. **Off-Platform Prevention**
Bloqueia tentativas de tirar conversas do app:

- Keywords: `"me chama no whatsapp"`, `"vamos combinar por fora"`
- Dom√≠nios: `wa.me`, `t.me`, `instagram.com/dm`
- Regex: Detecta men√ß√µes a WhatsApp, Telegram, Instagram

#### 4. **ML-based (Toxicity Thresholds)**
Limites de confian√ßa para cada categoria:

- **`minToxicity`**: Threshold geral de toxicidade (ex: 0.80 = 80%)
- **`categoriesThresholds`**: Limites espec√≠ficos por categoria

**Exemplo de fluxo:**
```
1. Quick Check (keywords, regex, dom√≠nios) ‚Üí Se bloqueia, retorna imediatamente
2. ML Scoring (ToxicityScoring.score) ‚Üí Gera [String: Double]
3. Threshold Check ‚Üí Se score >= threshold, bloqueia
4. Se passou em tudo ‚Üí Mensagem permitida
```

### Calibrando Thresholds

Use `suggest_thresholds.py` para encontrar valores √≥timos:

```bash
python scripts/suggest_thresholds.py \
  --checkpoint artifacts/run1/checkpoint-6 \
  --test data/test.jsonl
```

Output exemplo:
```
Categoria 'toxicity': threshold √≥timo = 0.82 (F1=0.89)
Categoria 'insult': threshold √≥timo = 0.75 (F1=0.84)
```

---

## Roadmap & Pr√≥ximos Passos

### ‚úÖ Implementado
- [x] Swift Package modular (`ChatAICore`)
- [x] Pipeline de ML (treino, avalia√ß√£o, exporta√ß√£o)
- [x] CoreML support
- [x] ONNX export
- [x] PII detection (BR)
- [x] Off-platform prevention
- [x] DemoApp SwiftUI
- [x] Multi-label classification (6 categorias)

### üöß Planejado
- [ ] **Android version** (Kotlin + Jetpack Compose + ONNX Runtime)
- [ ] **Tests**: Unit tests para `RuleEngine`, `ModerationService`
- [ ] **CI/CD**: GitHub Actions para treino autom√°tico
- [ ] **Telemetria**: Logs agregados (sem PII) para calibra√ß√£o cont√≠nua
- [ ] **A/B Testing**: Framework para testar diferentes thresholds
- [ ] **Multilingual**: Suporte a mais idiomas (EN, ES, FR)
- [ ] **Fine-tuning**: Dataset customizado para dom√≠nios espec√≠ficos
- [ ] **Explainability**: Highlight das palavras/scores que causaram bloqueio

---

## Contribuindo

1. Fork o reposit√≥rio
2. Crie uma branch: `git checkout -b feature/minha-feature`
3. Commit suas mudan√ßas: `git commit -m 'Adiciona nova feature'`
4. Push para a branch: `git push origin feature/minha-feature`
5. Abra um Pull Request

**Guidelines:**
- Siga conven√ß√µes Swift (SwiftLint)
- Adicione testes para novas features
- Atualize o README se adicionar novas funcionalidades

---

## Licen√ßa

MIT License ‚Äî Veja [LICENSE](LICENSE) para detalhes.

---

## Acknowledgments

**Stack:**
- [distilbert-base-multilingual-cased](https://huggingface.co/distilbert-base-multilingual-cased) ‚Äî HuggingFace
- [coremltools](https://github.com/apple/coremltools) ‚Äî Apple
- [ONNX](https://onnx.ai) ‚Äî Cross-platform ML

**Inspira√ß√£o:**
- [Perspective API](https://perspectiveapi.com) ‚Äî Google Jigsaw
- [Detoxify](https://github.com/unitaryai/detoxify) ‚Äî Unitary AI

---

**Desenvolvido por [Carlos Henrique](https://github.com/carloshpdoc)**
Estrutura SPM limpa, pronta para produ√ß√£o iOS e port√°vel para Android üöÄ
